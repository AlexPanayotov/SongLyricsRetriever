{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tamilsongs_lyric_crawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOAAUFsFnIEhe062Z2DA6Bb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VivekVinushanth/TamilSongsLyricsCorpus/blob/master/Tamilsongs_lyric_crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct4W8EZQ5_KL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McFDDTd5ay9K",
        "colab_type": "code",
        "outputId": "99bd61cc-18cc-4cef-da4f-eaf09ea107c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "!pip install scrapy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: zope.interface>=4.1.3 in /usr/local/lib/python3.6/dist-packages (from scrapy) (5.1.0)\n",
            "Requirement already satisfied: Twisted>=17.9.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (20.3.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.6.0)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.1.0)\n",
            "Requirement already satisfied: service-identity>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (18.1.0)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.6/dist-packages (from scrapy) (0.1.16)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.5.0)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (2.9.2)\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.22.0)\n",
            "Requirement already satisfied: pyOpenSSL>=16.2.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (19.1.0)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.6/dist-packages (from scrapy) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.1.3->scrapy) (47.1.1)\n",
            "Requirement already satisfied: Automat>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
            "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (17.5.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (19.3.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (19.0.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
            "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (2.0.2)\n",
            "Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from parsel>=1.5.0->scrapy) (1.12.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.9)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agZ21Y0vbJEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "class JsonWriterPipeline(object):\n",
        "\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open('quoteresult.jl', 'w')\n",
        "\n",
        "    def close_spider(self, spider):\n",
        "        self.file.close()\n",
        "\n",
        "    def process_item(self, item, spider):\n",
        "        line = json.dumps(dict(item)) + \"\\n\"\n",
        "        self.file.write(line)\n",
        "        return item"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP2dm7LNcsAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import logging\n",
        "# import scrapy\n",
        "# from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# class LyricSpider(scrapy.Spider):\n",
        "#     name = \"tamil_lyrics\"\n",
        "#     allowed_domains = [\"tamilpaa.com\"]\n",
        "#     start_urls = [\n",
        "#         # 'https://www.tamilpaa.com/tamil-movies-list-2020',\n",
        "#         # 'https://www.tamilpaa.com/tamil-movies-list-2019',\n",
        "#         # 'https://www.tamilpaa.com/tamil-movies-list-2018',\n",
        "#         'https://www.tamilpaa.com/tamil-movies-list-2017',\n",
        "  \n",
        "#     ]\n",
        "#     base_url = 'https://www.tamilpaa.com/'\n",
        "#     custom_settings = {\n",
        "#         'LOG_LEVEL': logging.WARNING,\n",
        "#         'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
        "#         'FEED_FORMAT':'json',                                 # Used for pipeline 2\n",
        "#         'FEED_URI': 'tamil_movies_lyric_2017.json' ,                    # Used for pipeline 2\n",
        "#         'FEED_EXPORT_ENCODING' : 'utf-8'\n",
        "#     }\n",
        "    \n",
        "#     # collecting movie url from starting pages\n",
        "#     def parse(self, response):\n",
        "#         for movie_url in response.xpath('//div[@class=\"image-overlay\"]/a/@href'):\n",
        "#           # print(\"movie_url: \")\n",
        "#           # print(movie_url.extract())\n",
        "#           movie_url = movie_url.extract()\n",
        "#           yield scrapy.Request(movie_url,callback=self.parse_all_songs)\n",
        "\n",
        "#     # collecting all songs url from movie page\n",
        "#     def parse_all_songs(self,movie_url_response):\n",
        "#         for all_song_url in movie_url_response.xpath('//div[@class= \"tab-content clearfix\"]/ul/li/a/@href'):\n",
        "#             # print(\"all_song_url: \")\n",
        "#             # print(all_song_url.extract())\n",
        "#             all_song_url = all_song_url.extract()\n",
        "#             yield scrapy.Request(all_song_url,callback=self.parse_lyric)\n",
        "\n",
        "#     # # reaching particular song and scrap details\n",
        "#     # def parse_every_lyric(self,all_song_url_response):\n",
        "#     #     for lyric_url in all_song_url_response.xpath('//div[@class=\"tabs-container bordered-all\"]/li/a/@href'):\n",
        "#     #         yield scrapy.Request(lyric_url, callback=self.parse_lyric)\n",
        "\n",
        "#     def parse_lyric(self, response):\n",
        "#             # print(\"parsing lyric......\")\n",
        "#             movie = response.xpath(\n",
        "#                  '//table[@class=\"standard mb-10px\"]/tbody/tr[1]/td[1]/text()').extract()\n",
        "#             # print(movie)\n",
        "#             movie = ' '.join([str(elem) for elem in movie])\n",
        "\n",
        "#             lyricist = response.xpath(\n",
        "#                 '//table[@class=\"standard mb-10px\"]/tbody/tr[2]/td[2]/text()').extract()\n",
        "#             # print(lyricist)\n",
        "#             lyricist = ' '.join([str(elem) for elem in lyricist])\n",
        "\n",
        "#             composer = response.xpath(\n",
        "#                 '//table[@class=\"standard mb-10px\"]//tbody/tr[1]/td[2]/text()').extract()\n",
        "#             # print(composer)\n",
        "#             composer = ' '.join([str(elem) for elem in composer])\n",
        "\n",
        "#             song = response.xpath(\n",
        "#                 '//div/h3[@class=\"colored-text-2\"]/text()').extract()\n",
        "#             # print(song)\n",
        "#             song = ' '.join([str(elem) for elem in song])\n",
        "            \n",
        "#             year = response.xpath(\n",
        "#                 '//table[@class=\"standard mb-10px\"]/tbody/tr[2]/td[1]/text()').extract()\n",
        "#             # print(year)\n",
        "#             year = ' '.join([str(elem) for elem in year])\n",
        "\n",
        "#             singers = response.xpath(\n",
        "#                 '//table[@class=\"standard mb-10px\"]/tbody/tr[3]/td/text()').extract()\n",
        "#             # print(singers)\n",
        "#             singers = ','.join([str(elem) for elem in singers])\n",
        "\n",
        "#             lyrics = response.xpath('//div[@class=\"info-box white-bg\"]/text()').extract()\n",
        "#             lyrics = ' '.join([str(elem) for elem in lyrics])\n",
        "#             lyrics=lyrics.replace('\\n', '')\n",
        "\n",
        "#             parsed_lyric =  {\n",
        "#             \"திரைப்படம்\": movie,\n",
        "#             \"பாடலாசிரியர்\": lyricist,\n",
        "#             \"இசையமைப்பாளர்\": composer,\n",
        "#             \"பாடல்\": song,\n",
        "#             \"வருடம்\": year,\n",
        "#             \"பாடியவர்கள்\": singers,\n",
        "#             \"பாடல்வரிகள்\": lyrics\n",
        "#             }\n",
        "\n",
        "#             with open('/content/drive/My Drive/Lyrics_2017/'+song+'.json', 'w', encoding='utf8') as json_file:\n",
        "#               # json.dump(person_dict, json_file)\n",
        "#               json.dump(parsed_lyric,json_file,ensure_ascii=False)\n",
        "                \n",
        "#             yield parsed_lyric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNQvNIaAEhgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "class LyricSpider(scrapy.Spider):\n",
        "    name = \"tamil_lyrics\"\n",
        "    allowed_domains = [\"tamilpaa.com\"]\n",
        "    start_urls = [\n",
        "        'https://www.tamilpaa.com/tamil-movies-list',\n",
        "    ]\n",
        "    base_url = 'https://www.tamilpaa.com/'\n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': logging.WARNING,\n",
        "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
        "        'FEED_FORMAT':'json',                                 # Used for pipeline 2\n",
        "        'FEED_URI': 'tamil_movies_lyric_all.json' ,                    # Used for pipeline 2\n",
        "        'FEED_EXPORT_ENCODING' : 'utf-8'\n",
        "    }\n",
        "    \n",
        "    # collecting movie url from starting pages\n",
        "    def parse(self, response):\n",
        "        tables = response.xpath('//div[@class=\"one-col mb-5px\"]/table[@class=\"standard mb-50px\"]/tr/td[1]/a/@href')\n",
        "        # print(tables)\n",
        "        for movie_url in tables[1:]:\n",
        "          # print(\"movie_url: \")\n",
        "          # print(movie_url.extract())\n",
        "          movie_url = movie_url.extract()\n",
        "          yield scrapy.Request(movie_url,callback=self.parse_all_songs)\n",
        "\n",
        "    # collecting all songs url from movie page\n",
        "    def parse_all_songs(self,movie_url_response):\n",
        "        for all_song_url in movie_url_response.xpath('//div[@class= \"tab-content clearfix\"]/ul/li/a/@href'):\n",
        "            # print(\"all_song_url: \")\n",
        "            # print(all_song_url.extract())\n",
        "            all_song_url = all_song_url.extract()\n",
        "            yield scrapy.Request(all_song_url,callback=self.parse_lyric)\n",
        "\n",
        "    # # reaching particular song and scrap details\n",
        "    # def parse_every_lyric(self,all_song_url_response):\n",
        "    #     for lyric_url in all_song_url_response.xpath('//div[@class=\"tabs-container bordered-all\"]/li/a/@href'):\n",
        "    #         yield scrapy.Request(lyric_url, callback=self.parse_lyric)\n",
        "\n",
        "    def parse_lyric(self, response):\n",
        "            # print(\"parsing lyric......\")\n",
        "            movie = response.xpath(\n",
        "                 '//table[@class=\"standard mb-10px\"]/tbody/tr[1]/td[1]/text()').extract()\n",
        "            # print(movie)\n",
        "            movie = ' '.join([str(elem) for elem in movie])\n",
        "\n",
        "            lyricist = response.xpath(\n",
        "                '//table[@class=\"standard mb-10px\"]/tbody/tr[2]/td[2]/text()').extract()\n",
        "            # print(lyricist)\n",
        "            lyricist = ' '.join([str(elem) for elem in lyricist])\n",
        "\n",
        "            composer = response.xpath(\n",
        "                '//table[@class=\"standard mb-10px\"]//tbody/tr[1]/td[2]/text()').extract()\n",
        "            # print(composer)\n",
        "            composer = ' '.join([str(elem) for elem in composer])\n",
        "\n",
        "            song = response.xpath(\n",
        "                '//div/h3[@class=\"colored-text-2\"]/text()').extract()\n",
        "            # print(song)\n",
        "            song = ' '.join([str(elem) for elem in song])\n",
        "            \n",
        "            year = response.xpath(\n",
        "                '//table[@class=\"standard mb-10px\"]/tbody/tr[2]/td[1]/text()').extract()\n",
        "            # print(year)\n",
        "            year = ' '.join([str(elem) for elem in year])\n",
        "\n",
        "            singers = response.xpath(\n",
        "                '//table[@class=\"standard mb-10px\"]/tbody/tr[3]/td/text()').extract()\n",
        "            # print(singers)\n",
        "            singers = ','.join([str(elem) for elem in singers])\n",
        "\n",
        "            lyrics = response.xpath('//div[@class=\"info-box white-bg\"]/text()').extract()\n",
        "            lyrics = ' '.join([str(elem) for elem in lyrics])\n",
        "            lyrics=lyrics.replace('\\n','')\n",
        "\n",
        "            parsed_lyric =  {\n",
        "            \"திரைப்படம்\": movie,\n",
        "            \"பாடலாசிரியர்\": lyricist,\n",
        "            \"இசையமைப்பாளர்\": composer,\n",
        "            \"பாடல்\": song,\n",
        "            \"வருடம்\": year,\n",
        "            \"பாடியவர்கள்\": singers,\n",
        "            \"பாடல்வரிகள்\": lyrics\n",
        "            }\n",
        "\n",
        "            with open('/content/drive/My Drive/Lyrics_Others/'+song+'.json', 'w', encoding='utf8') as json_file:\n",
        "              # json.dump(person_dict, json_file)\n",
        "              json.dump(parsed_lyric,json_file,ensure_ascii=False)\n",
        "                \n",
        "            yield parsed_lyric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQaDLbdQcviN",
        "colab_type": "code",
        "outputId": "34aff061-2837-4f95-a511-a9834f71a498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "process = CrawlerProcess({\n",
        "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
        "})\n",
        "\n",
        "process.crawl(LyricSpider)\n",
        "process.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-13 04:03:38 [scrapy.utils.log] INFO: Scrapy 2.1.0 started (bot: scrapybot)\n",
            "2020-06-13 04:03:38 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Apr 18 2020, 01:56:04) - [GCC 8.4.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Linux-4.19.104+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2020-06-13 04:03:38 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2020-06-13 04:03:38 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'LOG_LEVEL': 30,\n",
            " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
            "/usr/local/lib/python3.6/dist-packages/scrapy/extensions/feedexport.py:210: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
            "  exporter = cls(crawler)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}