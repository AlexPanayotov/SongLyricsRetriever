{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tamilsongs_lyric_crawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNizl7xrH2PW9JJtdew/dze",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VivekVinushanth/TamilSongsLyricsCorpus/blob/master/Tamilsongs_lyric_crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct4W8EZQ5_KL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McFDDTd5ay9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install scrapy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agZ21Y0vbJEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "class JsonWriterPipeline(object):\n",
        "\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open('quoteresult.jl', 'w')\n",
        "\n",
        "    def close_spider(self, spider):\n",
        "        self.file.close()\n",
        "\n",
        "    def process_item(self, item, spider):\n",
        "        line = json.dumps(dict(item)) + \"\\n\"\n",
        "        self.file.write(line)\n",
        "        return item"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP2dm7LNcsAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "class LyricSpider(scrapy.Spider):\n",
        "    name = \"tamil_lyrics\"\n",
        "    allowed_domains = [\"tamilpaa.com\"]\n",
        "    start_urls = [\n",
        "        # 'https://www.tamilpaa.com/tamil-movies-list-2020',\n",
        "        'https://www.tamilpaa.com/tamil-movies-list-2019',\n",
        "        'https://www.tamilpaa.com/tamil-movies-list-2018',\n",
        "        'https://www.tamilpaa.com/tamil-movies-list-2017',\n",
        "  \n",
        "    ]\n",
        "    base_url = 'https://www.tamilpaa.com/'\n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': logging.WARNING,\n",
        "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
        "        'FEED_FORMAT':'json',                                 # Used for pipeline 2\n",
        "        'FEED_URI': 'tamil_movies_lyric_all.json' ,                    # Used for pipeline 2\n",
        "        'FEED_EXPORT_ENCODING' : 'utf-8'\n",
        "    }\n",
        "    \n",
        "    # collecting movie url from starting pages\n",
        "    def parse(self, response):\n",
        "        for movie_url in response.xpath('//div[@class=\"image-overlay\"]/a/@href'):\n",
        "          # print(\"movie_url: \")\n",
        "          # print(movie_url.extract())\n",
        "          movie_url = movie_url.extract()\n",
        "          yield scrapy.Request(movie_url,callback=self.parse_all_songs)\n",
        "\n",
        "    # collecting all songs url from movie page\n",
        "    def parse_all_songs(self,movie_url_response):\n",
        "        for all_song_url in movie_url_response.xpath('//div[@class= \"tab-content clearfix\"]/ul/li/a/@href'):\n",
        "            # print(\"all_song_url: \")\n",
        "            # print(all_song_url.extract())\n",
        "            all_song_url = all_song_url.extract()\n",
        "            yield scrapy.Request(all_song_url,callback=self.parse_lyric)\n",
        "\n",
        "    # # reaching particular song and scrap details\n",
        "    # def parse_every_lyric(self,all_song_url_response):\n",
        "    #     for lyric_url in all_song_url_response.xpath('//div[@class=\"tabs-container bordered-all\"]/li/a/@href'):\n",
        "    #         yield scrapy.Request(lyric_url, callback=self.parse_lyric)\n",
        "\n",
        "    def parse_lyric(self, response):\n",
        "            # print(\"parsing lyric......\")\n",
        "            movie = response.xpath(\n",
        "                 '//table[@class=\"standard mb-10px\"]/tbody/tr[1]/td[1]/text()').extract()\n",
        "            # print(movie)\n",
        "            movie = ' '.join([str(elem) for elem in movie])\n",
        "\n",
        "            lyricist = response.xpath(\n",
        "                '//table[@class=\"standard mb-10px\"]/tbody/tr[2]/td[2]/text()').extract()\n",
        "            # print(lyricist)\n",
        "            lyricist = ' '.join([str(elem) for elem in lyricist])\n",
        "\n",
        "            composer = response.xpath(\n",
        "                '//table[@class=\"standard mb-10px\"]//tbody/tr[1]/td[2]/text()').extract()\n",
        "            # print(composer)\n",
        "            composer = ' '.join([str(elem) for elem in composer])\n",
        "\n",
        "            song = response.xpath(\n",
        "                '//div/h3[@class=\"colored-text-2\"]/text()').extract()\n",
        "            # print(song)\n",
        "            song = ' '.join([str(elem) for elem in song])\n",
        "            \n",
        "            year = response.xpath(\n",
        "                '//table[@class=\"standard mb-10px\"]/tbody/tr[2]/td[1]/text()').extract()\n",
        "            # print(year)\n",
        "            year = ' '.join([str(elem) for elem in year])\n",
        "\n",
        "            singers = response.xpath(\n",
        "                '//table[@class=\"standard mb-10px\"]/tbody/tr[3]/td/text()').extract()\n",
        "            # print(singers)\n",
        "            singers = ','.join([str(elem) for elem in singers])\n",
        "\n",
        "            lyrics = response.xpath('//div[@class=\"info-box white-bg\"]/text()').extract()\n",
        "            lyrics = ' '.join([str(elem) for elem in lyrics])\n",
        "            lyrics=lyrics.replace('\\n','')\n",
        "\n",
        "            parsed_lyric =  {\n",
        "            \"Movie\": movie,\n",
        "            \"Lyricist\": lyricist,\n",
        "            \"Composer\": composer,\n",
        "            \"Song\": song,\n",
        "            \"Year\": year,\n",
        "            \"Singers\": singers,\n",
        "            \"lyric\": lyrics\n",
        "            }\n",
        "\n",
        "            with open('/content/drive/My Drive/Lyrics_Others/'+song+'.json', 'w', encoding='utf8') as json_file:\n",
        "              # json.dump(person_dict, json_file)\n",
        "              json.dump(parsed_lyric,json_file,ensure_ascii=False)\n",
        "                \n",
        "            yield parsed_lyric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNQvNIaAEhgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "class LyricSpider(scrapy.Spider):\n",
        "    name = \"tamil_lyrics\"\n",
        "    allowed_domains = [\"tamilpaa.com\"]\n",
        "    start_urls = [\n",
        "        'https://www.tamilpaa.com/tamil-movies-list',\n",
        "    ]\n",
        "    base_url = 'https://www.tamilpaa.com/'\n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': logging.WARNING,\n",
        "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
        "        'FEED_FORMAT':'json',                                 # Used for pipeline 2\n",
        "        'FEED_URI': 'tamil_movies_lyric_all.json' ,                    # Used for pipeline 2\n",
        "        'FEED_EXPORT_ENCODING' : 'utf-8'\n",
        "    }\n",
        "    \n",
        "    # collecting movie url from starting pages\n",
        "    def parse(self, response):\n",
        "        tables = response.xpath('//div[@class=\"one-col mb-5px\"]/table[@class=\"standard mb-50px\"]/tr/td[1]/a/@href')\n",
        "        # print(tables)\n",
        "        for movie_url in tables[1:]:\n",
        "          # print(\"movie_url: \")\n",
        "          # print(movie_url.extract())\n",
        "          movie_url = movie_url.extract()\n",
        "          yield scrapy.Request(movie_url,callback=self.parse_all_songs)\n",
        "\n",
        "    # collecting all songs url from movie page\n",
        "    def parse_all_songs(self,movie_url_response):\n",
        "        for all_song_url in movie_url_response.xpath('//div[@class= \"tab-content clearfix\"]/ul/li/a/@href'):\n",
        "            # print(\"all_song_url: \")\n",
        "            # print(all_song_url.extract())\n",
        "            all_song_url = all_song_url.extract()\n",
        "            yield scrapy.Request(all_song_url,callback=self.parse_lyric)\n",
        "\n",
        "    # # reaching particular song and scrap details\n",
        "    # def parse_every_lyric(self,all_song_url_response):\n",
        "    #     for lyric_url in all_song_url_response.xpath('//div[@class=\"tabs-container bordered-all\"]/li/a/@href'):\n",
        "    #         yield scrapy.Request(lyric_url, callback=self.parse_lyric)\n",
        "\n",
        "    def parse_lyric(self, response):\n",
        "            # print(\"parsing lyric......\")\n",
        "            movie = response.xpath(\n",
        "                 '//table[@class=\"standard mb-10px\"]/tbody/tr[1]/td[1]/text()').extract()\n",
        "            # print(movie)\n",
        "            movie = ' '.join([str(elem) for elem in movie])\n",
        "\n",
        "            lyricist = response.xpath(\n",
        "                '//table[@class=\"standard mb-10px\"]/tbody/tr[2]/td[2]/text()').extract()\n",
        "            # print(lyricist)\n",
        "            lyricist = ' '.join([str(elem) for elem in lyricist])\n",
        "\n",
        "            composer = response.xpath(\n",
        "                '//table[@class=\"standard mb-10px\"]//tbody/tr[1]/td[2]/text()').extract()\n",
        "            # print(composer)\n",
        "            composer = ' '.join([str(elem) for elem in composer])\n",
        "\n",
        "            song = response.xpath(\n",
        "                '//div/h3[@class=\"colored-text-2\"]/text()').extract()\n",
        "            # print(song)\n",
        "            song = ' '.join([str(elem) for elem in song])\n",
        "            \n",
        "            year = response.xpath(\n",
        "                '//table[@class=\"standard mb-10px\"]/tbody/tr[2]/td[1]/text()').extract()\n",
        "            # print(year)\n",
        "            year = ' '.join([str(elem) for elem in year])\n",
        "\n",
        "            singers = response.xpath(\n",
        "                '//table[@class=\"standard mb-10px\"]/tbody/tr[3]/td/text()').extract()\n",
        "            # print(singers)\n",
        "            singers = ','.join([str(elem) for elem in singers])\n",
        "\n",
        "            lyrics = response.xpath('//div[@class=\"info-box white-bg\"]/text()').extract()\n",
        "            lyrics = ' '.join([str(elem) for elem in lyrics])\n",
        "            lyrics=lyrics.replace('\\n','')\n",
        "\n",
        "            parsed_lyric =  {\n",
        "            \"Movie\": movie,\n",
        "            \"Lyricist\": lyricist,\n",
        "            \"Composer\": composer,\n",
        "            \"Song\": song,\n",
        "            \"Year\": year,\n",
        "            \"Singers\": singers,\n",
        "            \"lyric\": lyrics\n",
        "            }\n",
        "\n",
        "            with open('/content/drive/My Drive/Lyrics_Others/'+song+'.json', 'w', encoding='utf8') as json_file:\n",
        "              # json.dump(person_dict, json_file)\n",
        "              json.dump(parsed_lyric,json_file,ensure_ascii=False)\n",
        "                \n",
        "            yield parsed_lyric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQaDLbdQcviN",
        "colab_type": "code",
        "outputId": "e69b2992-fe81-4e54-ce42-4ab115f586ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "process = CrawlerProcess({\n",
        "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
        "})\n",
        "\n",
        "process.crawl(LyricSpider)\n",
        "process.start()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-12 20:41:45 [scrapy.utils.log] INFO: Scrapy 2.1.0 started (bot: scrapybot)\n",
            "2020-06-12 20:41:45 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Apr 18 2020, 01:56:04) - [GCC 8.4.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Linux-4.19.104+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2020-06-12 20:41:45 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2020-06-12 20:41:45 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'LOG_LEVEL': 30,\n",
            " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
            "/usr/local/lib/python3.6/dist-packages/scrapy/extensions/feedexport.py:210: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
            "  exporter = cls(crawler)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}